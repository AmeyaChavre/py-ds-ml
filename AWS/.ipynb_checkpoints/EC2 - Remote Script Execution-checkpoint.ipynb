{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remotely Running Scripts on EC2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create an IAM User. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### 2. Create a role with AmazonEC2RoleforSSM attached using root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Login as IAM user and Launch Instances and change the IAM role to \"AmazonEC2RoleforSSM\"  while  configuring the settings for EC2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. If there are instances that are already running then we can change the IAM role on the fly using Instance Settings for EC2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. After attaching this role you must be able to see the list of managed instances in AWS Systems Manager >> Fleet Manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Now go back to \"Run Command\" select the document with AWSRunShellScript and select the instances . We can also enable S3 bucket and Cloudwatch here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Paste the below command and click \"Run Command\" button."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sudo su -\n",
    "mkdir -p /test/test_file/1\n",
    "cd /test/test_file/1\n",
    "fallocate -l 2GB target.txt\n",
    "\n",
    "(echo `curl -s http://169.254.169.254/latest/meta-data/instance-id` \",\" `curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone` \",\" `du -sm /test/test_file/1/target.txt | cut -f1`;) > \"$(curl -s http://169.254.169.254/latest/meta-data/instance-id).csv\" && (aws s3 cp *.csv s3://<randombucketname>/)\n",
    "\n",
    "-- This sends a .csv file to mentioned bucket with filename <instance-id>.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Go to S3 and download all CSV files from the bucket. Search through tag i-*. (Here we can use Athena or Lambdas we can also use Boto3 to read through S3 objects , there are many such alternatives that can be explored)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. To combine the CSV's run the following commands: "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cat *.csv > combined.csv (Linux/Mac)\n",
    "copy *.csv combined.csv (Windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Then after adding columns \"Instance-Id\",\"Region\",\"Size\" to the csv we can read and analyze using Pandas,Numpy or tools such as Tableau and compare the file size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
