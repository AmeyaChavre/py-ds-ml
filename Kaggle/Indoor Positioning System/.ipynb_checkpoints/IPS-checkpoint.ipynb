{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indoor Positioning System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import KFold, GroupKFold\n",
    "import lightgbm as lgb\n",
    "\n",
    "import psutil\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "N_SPLITS = 5\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "LOG_PATH = Path(\"./log/\")\n",
    "LOG_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    t0 = time.time()\n",
    "    p = psutil.Process(os.getpid())\n",
    "    m0 = p.memory_info()[0] / 2. ** 30\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        m1 = p.memory_info()[0] / 2. ** 30\n",
    "        delta = m1 - m0\n",
    "        sign = '+' if delta >= 0 else '-'\n",
    "        delta = math.fabs(delta)\n",
    "        print(f\"[{m1:.1f}GB({sign}{delta:.1f}GB): {time.time() - t0:.3f}sec] {name}\", file=sys.stderr)\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    \n",
    "def comp_metric(xhat, yhat, fhat, x, y, f):\n",
    "    intermediate = np.sqrt(np.power(xhat-x, 2) + np.power(yhat-y, 2)) + 15 * np.abs(fhat-f)\n",
    "    return intermediate.sum()/xhat.shape[0]\n",
    "\n",
    "\n",
    "def score_log(df: pd.DataFrame, num_files: int, nam_file: str, data_shape: tuple, n_fold: int, seed: int, mpe: float):\n",
    "    score_dict = {'n_files': num_files, 'file_name': nam_file, 'shape': data_shape, 'fold': n_fold, 'seed': seed, 'score': mpe}\n",
    "    # noinspection PyTypeChecker\n",
    "    df = pd.concat([df, pd.DataFrame.from_dict([score_dict])])\n",
    "    df.to_csv(LOG_PATH / f\"log_score.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "\n",
    "feature_dir = \"../input/indoor-navigation-and-location-wifi-features\"\n",
    "train_files = sorted(glob.glob(os.path.join(feature_dir, '*_train.csv')))\n",
    "test_files = sorted(glob.glob(os.path.join(feature_dir, '*_test.csv')))\n",
    "subm = pd.read_csv('../input/indoor-location-navigation/sample_submission.csv', index_col=0)\n",
    "\n",
    "\n",
    "lgb_params = {'objective': 'root_mean_squared_error',\n",
    "              'boosting_type': 'gbdt',\n",
    "              'n_estimators': 50000,\n",
    "              'learning_rate': 0.1,\n",
    "              'num_leaves': 90,\n",
    "              'colsample_bytree': 0.4,\n",
    "              'subsample': 0.6,\n",
    "              'subsample_freq': 2,\n",
    "              'bagging_seed': SEED,\n",
    "              'reg_alpha': 8,\n",
    "              'reg_lambda': 2,\n",
    "              'random_state': SEED,\n",
    "              'n_jobs': -1\n",
    "              }\n",
    "\n",
    "lgb_f_params = {'objective': 'multiclass',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'n_estimators': 50000,\n",
    "                'learning_rate': 0.1,\n",
    "                'num_leaves': 90,\n",
    "                'colsample_bytree': 0.4,\n",
    "                'subsample': 0.6,\n",
    "                'subsample_freq': 2,\n",
    "                'bagging_seed': SEED,\n",
    "                'reg_alpha': 8,\n",
    "                'reg_lambda': 2,\n",
    "                'random_state': SEED,\n",
    "                'n_jobs': -1\n",
    "                }\n",
    "\n",
    "\n",
    "score_df = pd.DataFrame()\n",
    "oof = list()\n",
    "predictions = list()\n",
    "for n_files, file in enumerate(train_files):\n",
    "    data = pd.read_csv(file, index_col=0)\n",
    "    test_data = pd.read_csv(test_files[n_files], index_col=0)\n",
    "\n",
    "    oof_x, oof_y, oof_f = np.zeros(data.shape[0]), np.zeros(data.shape[0]), np.zeros(data.shape[0])\n",
    "    preds_x, preds_y = 0, 0\n",
    "    preds_f_arr = np.zeros((test_data.shape[0], N_SPLITS))\n",
    "\n",
    "    gkf = GroupKFold(n_splits=N_SPLITS)\n",
    "    for fold, (trn_idx, val_idx) in enumerate(gkf.split(data.iloc[:, :-4], groups=data.iloc[:, -1])):\n",
    "        X_train = data.iloc[trn_idx, :-4]\n",
    "        y_trainx = data.iloc[trn_idx, -4]\n",
    "        y_trainy = data.iloc[trn_idx, -3]\n",
    "        y_trainf = data.iloc[trn_idx, -2]\n",
    "\n",
    "        X_valid = data.iloc[val_idx, :-4]\n",
    "        y_validx = data.iloc[val_idx, -4]\n",
    "        y_validy = data.iloc[val_idx, -3]\n",
    "        y_validf = data.iloc[val_idx, -2]\n",
    "\n",
    "        modelx = lgb.LGBMRegressor(**lgb_params)\n",
    "        with timer(\"fit X\"):\n",
    "            modelx.fit(X_train, y_trainx,\n",
    "                       eval_set=[(X_valid, y_validx)],\n",
    "                       eval_metric='rmse',\n",
    "                       verbose=False,\n",
    "                       early_stopping_rounds=20\n",
    "                       )\n",
    "\n",
    "        modely = lgb.LGBMRegressor(**lgb_params)\n",
    "        with timer(\"fit Y\"):\n",
    "            modely.fit(X_train, y_trainy,\n",
    "                       eval_set=[(X_valid, y_validy)],\n",
    "                       eval_metric='rmse',\n",
    "                       verbose=False,\n",
    "                       early_stopping_rounds=20\n",
    "                       )\n",
    "\n",
    "        modelf = lgb.LGBMClassifier(**lgb_f_params)\n",
    "        with timer(\"fit F\"):\n",
    "            modelf.fit(X_train, y_trainf,\n",
    "                       eval_set=[(X_valid, y_validf)],\n",
    "                       eval_metric='multi_logloss',\n",
    "                       verbose=False,\n",
    "                       early_stopping_rounds=20\n",
    "                       )\n",
    "\n",
    "        oof_x[val_idx] = modelx.predict(X_valid)\n",
    "        oof_y[val_idx] = modely.predict(X_valid)\n",
    "        oof_f[val_idx] = modelf.predict(X_valid).astype(int)\n",
    "\n",
    "        preds_x += modelx.predict(test_data.iloc[:, :-1]) / N_SPLITS\n",
    "        preds_y += modely.predict(test_data.iloc[:, :-1]) / N_SPLITS\n",
    "        preds_f_arr[:, fold] = modelf.predict(test_data.iloc[:, :-1]).astype(int)\n",
    "\n",
    "        score = comp_metric(oof_x[val_idx], oof_y[val_idx], oof_f[val_idx],\n",
    "                            y_validx.to_numpy(), y_validy.to_numpy(), y_validf.to_numpy())\n",
    "        print(f\"fold {fold}: mean position error {score}\")\n",
    "        score_df = score_log(score_df, n_files, os.path.basename(file), data.shape, fold, SEED, score)\n",
    "\n",
    "    print(\"*+\"*40)\n",
    "    print(f\"file #{n_files}, shape={data.shape}, name={os.path.basename(file)}\")\n",
    "    score = comp_metric(oof_x, oof_y, oof_f,\n",
    "                        data.iloc[:, -4].to_numpy(), data.iloc[:, -3].to_numpy(), data.iloc[:, -2].to_numpy())\n",
    "    oof.append(score)\n",
    "    print(f\"mean position error {score}\")\n",
    "    print(\"*+\"*40)\n",
    "    score_df = score_log(score_df, n_files, os.path.basename(file), data.shape, 999, SEED, score)\n",
    "\n",
    "    preds_f_mode = stats.mode(preds_f_arr, axis=1)\n",
    "    preds_f = preds_f_mode[0].astype(int).reshape(-1)\n",
    "    test_preds = pd.DataFrame(np.stack((preds_f, preds_x, preds_y))).T\n",
    "    test_preds.columns = subm.columns\n",
    "    test_preds.index = test_data[\"site_path_timestamp\"]\n",
    "    test_preds[\"floor\"] = test_preds[\"floor\"].astype(int)\n",
    "    predictions.append(test_preds)\n",
    "\n",
    "\n",
    "all_preds = pd.concat(predictions)\n",
    "all_preds = all_preds.reindex(subm.index)\n",
    "all_preds.to_csv('submission.csv')\n",
    "#Source : Kaggle Dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
